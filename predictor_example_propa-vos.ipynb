{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc06a1f",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nye0/SAM-Med2D/blob/main/predictor_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffb09a",
   "metadata": {
    "id": "c0b71431"
   },
   "source": [
    "## Environment Set-up\n",
    "edit from [sam colab](https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb#scrollTo=MTeAdX_mHwAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e0ca4",
   "metadata": {
    "id": "47e5a78f"
   },
   "source": [
    "\n",
    "\n",
    "If you're running this notebook locally using Jupyter, please clone `SAM-Med2D` into a directory named `SAM_Med2D`. Note that you do **not** need to install `segment_anything` in your local environment, as `SAM-Med2D` and `SAM` share function names that could lead to conflicts.\n",
    "\n",
    "For Google Colab users: Set `using_colab=True` in the cell below before executing it. Although you can select 'GPU' under 'Edit' -> 'Notebook Settings' -> 'Hardware Accelerator', this notebook is designed to run efficiently in a CPU environment as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f6d46",
   "metadata": {
    "id": "b4a4b25c"
   },
   "source": [
    "# SAM-Med2D generates predicted object masks based on prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a94748",
   "metadata": {
    "id": "69b28288"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "import SimpleITK as sitk\n",
    "from skimage.measure import label, regionprops\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25534d3",
   "metadata": {
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.array([0, 1, 0, 0.5])\n",
    "    else:\n",
    "        color = np.array([1, 0, 0, 0.5])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=100):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='.', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='.', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b4bf4",
   "metadata": {
    "id": "98b228b8"
   },
   "source": [
    "## Load SAM-Med2D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8cc127",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e28150b",
    "outputId": "265f718c-79a0-4a6e-b1fe-759b21f20d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "*******load /volume/willy-dev/sota/SAM-Med2D/sam-med2d_b.pth\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.predictor_sammed import SammedPredictor\n",
    "from argparse import Namespace\n",
    "args = Namespace()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.image_size = 256\n",
    "args.encoder_adapter = True\n",
    "args.sam_checkpoint = \"/volume/willy-dev/sota/SAM-Med2D/sam-med2d_b.pth\"\n",
    "model = sam_model_registry[\"vit_b\"](args).to(device)\n",
    "predictor = SammedPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dfff362-a25b-4a2f-8d43-41f18a7876b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******load /volume/willy-dev/sota/SAM-Med2D/sam_vit_b_01ec64.pth\n"
     ]
    }
   ],
   "source": [
    "args = Namespace()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.image_size = 1024\n",
    "args.encoder_adapter = False\n",
    "args.sam_checkpoint = \"/volume/willy-dev/sota/SAM-Med2D/sam_vit_b_01ec64.pth\"\n",
    "model2 = sam_model_registry[\"vit_b\"](args).to(device)\n",
    "predictor2 = SammedPredictor(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76e0f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 76.5MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:00<00:00, 59.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import open_dict\n",
    "from hydra import compose, initialize\n",
    "from cutie.model.cutie import CUTIE\n",
    "from cutie.inference.inference_core import InferenceCore\n",
    "from cutie.inference.utils.args_utils import get_dataset_cfg\n",
    "from gui.interactive_utils import image_to_torch, torch_prob_to_numpy_mask, index_numpy_to_one_hot_torch, overlay_davis\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    initialize(version_base='1.3.2', config_path=\"Cutie/cutie/config\", job_name=\"eval_config\")\n",
    "    cfg = compose(config_name=\"eval_config\")\n",
    "\n",
    "    with open_dict(cfg):\n",
    "        cfg['weights'] = '/volume/willy-dev/sota/SAM-Med2D/Cutie/weights/cutie-base-mega.pth'\n",
    "\n",
    "    data_cfg = get_dataset_cfg(cfg)\n",
    "\n",
    "    # Load the network weights\n",
    "    cutie = CUTIE(cfg).cuda().eval()\n",
    "    model_weights = torch.load(cfg.weights)\n",
    "    cutie.load_weights(model_weights)\n",
    "\n",
    "processor = InferenceCore(cutie, cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f907de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e0cdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice(img, mask, index):\n",
    "    \n",
    "    slce = img[index].astype(float)\n",
    "    slce -= slce.min()\n",
    "    slce /= slce.max()\n",
    "    slce *= 255\n",
    "    slce = slce.astype(np.uint8)\n",
    "    slce = np.stack([slce, slce, slce], axis=2)\n",
    "\n",
    "    gt = mask[index]\n",
    "    \n",
    "    return np.copy(slce), np.copy(gt)\n",
    "\n",
    "def get_predict(img, predictor, point=None, box=None):\n",
    "    \n",
    "    predictor.set_image(slce)\n",
    "    \n",
    "    if point is not None:\n",
    "        input_point = np.array([point])\n",
    "        input_label = np.array([1])\n",
    "    else:\n",
    "        input_point, input_label = None, None\n",
    "    \n",
    "    if box is not None:\n",
    "        box = np.array([box])\n",
    "    else:\n",
    "        box = None\n",
    "        \n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        box=box,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    \n",
    "    return masks[0]\n",
    "    \n",
    "    return predictor.model.image_encoder(x).flatten().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3655dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_mask(slce, init_center, predictor, last_slce=None, centroid=None):\n",
    "    \n",
    "    if last_slce is None:\n",
    "        point = [init_center[2], init_center[1]]\n",
    "    else:\n",
    "        flow = cv2.calcOpticalFlowFarneback(last_slce[:,:,0], slce[:,:,0], None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        point = [int(centroid[1]), int(centroid[0])] + flow[int(centroid[1]), int(centroid[0]), [1, 0]].astype(int)\n",
    "        \n",
    "    predict_mask = get_predict(slce, predictor, point = point)\n",
    "    predict_mask = label(predict_mask.astype(int))\n",
    "    \n",
    "    target_pixel = predict_mask[point[1], point[0]]\n",
    "\n",
    "    predict_mask[predict_mask != target_pixel] = 0\n",
    "    predict_mask[predict_mask == target_pixel] = 1\n",
    "    \n",
    "    mask_prop = regionprops(predict_mask)[0]\n",
    "    box, centroid = mask_prop.bbox, mask_prop.centroid\n",
    "\n",
    "#     crop_img = slce[box[0]:box[2], box[1]:box[3]]\n",
    "#     crop_encode = get_encode(crop_img, predictor)\n",
    "    \n",
    "    return predict_mask, slce, target_pixel, centroid, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d806b0-b558-4c2a-8675-f10df2322643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(slce, predict_mask, gt, box, fn):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(slce)\n",
    "    show_mask(gt, plt.gca(), random_color=True)\n",
    "    show_mask(predict_mask, plt.gca())\n",
    "    show_box(box, plt.gca())\n",
    "    plt.axis('off')\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"output/{fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3d49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(slce, predict_mask, gt, points, labels, fn):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(slce)\n",
    "    # show_mask(gt, plt.gca(), random_color=True)\n",
    "    show_mask(predict_mask, plt.gca())\n",
    "    show_points(points, labels, plt.gca())\n",
    "    plt.axis('off')\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"output/{fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce80e438-538a-4ebf-9262-1946edf48aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [06:07,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import ops\n",
    "\n",
    "dices = []\n",
    "dicesb = []\n",
    "\n",
    "for j, di in tqdm(enumerate(glob.glob('/volume/open-dataset-ssd/ai99/gen_data/neuroma/*'))):\n",
    "    \n",
    "    try:\n",
    "        img = sitk.ReadImage(f'{di}/axc.nii.gz')\n",
    "        img = sitk.GetArrayFromImage(img)\n",
    "\n",
    "        mask = sitk.ReadImage(f'{di}/seg.nii.gz')\n",
    "        mask = sitk.GetArrayFromImage(mask)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # if img.shape[0] < 100:\n",
    "        # continue\n",
    "\n",
    "    mask = label(mask)\n",
    "\n",
    "    for i, prop in enumerate(regionprops(mask)):\n",
    "        \n",
    "        dices_tp = 0\n",
    "        dices_pred = 0\n",
    "        \n",
    "        init_center = ((np.array(prop.bbox[:3]) + np.array(prop.bbox[3:]))/2).astype(int)\n",
    "\n",
    "        z = init_center[0]\n",
    "        slce, gt = get_slice(img, mask, z)\n",
    "        \n",
    "        gt[gt!=(i+1)] = 0\n",
    "        gt[gt==(i+1)] = 1\n",
    "        gt = F.interpolate(torch.tensor(gt).float().unsqueeze(0).unsqueeze(0), (512, 512))[0][0].int().numpy()\n",
    "        \n",
    "        predict_mask, slce, target_pixel, centroid, crop_encode = get_predict_mask(slce, init_center, predictor)\n",
    "        \n",
    "        frame_torch = image_to_torch(slce, device=device)\n",
    "        mask_torch = index_numpy_to_one_hot_torch(predict_mask, 2).to(device)\n",
    "        \n",
    "        mask_torch = F.interpolate(mask_torch.unsqueeze(0), (512, 512))[0]\n",
    "        init_mask_torch = torch.zeros(4, 512, 512).cuda()\n",
    "        init_mask_torch[0] = mask_torch[1]\n",
    "        \n",
    "        init_frame_torch = F.interpolate(frame_torch.unsqueeze(0), (480, 480))[0]\n",
    "        init_mask_torch = F.interpolate(init_mask_torch.unsqueeze(0), (480, 480))[0]\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            processor.clear_memory()\n",
    "            prediction = processor.step(init_frame_torch, init_mask_torch, idx_mask=False)\n",
    "\n",
    "        prediction = torch_prob_to_numpy_mask(prediction)\n",
    "        prediction = torch.tensor(prediction).float().unsqueeze(0).unsqueeze(0)\n",
    "        prediction = F.interpolate(prediction, (512, 512))[0][0].cpu().numpy()\n",
    "\n",
    "        # visualize(slce, prediction, gt, [[init_center[1], init_center[2]]], f\"{os.path.basename(di)}_{z}.jpg\")\n",
    "\n",
    "        dices_tp += (gt*prediction).sum()\n",
    "        dices_pred += prediction.sum()\n",
    "        dice_ = 2 * (gt*prediction).sum()/(gt.sum() + prediction.sum())\n",
    "        dices.append(dice_)\n",
    "        \n",
    "        z = init_center[0] + 1\n",
    "        while z < img.shape[0]:\n",
    "            \n",
    "            slce, gt = get_slice(img, mask, z)\n",
    "            \n",
    "            gt[gt!=(i+1)] = 0\n",
    "            gt[gt==(i+1)] = 1\n",
    "            # gt = F.interpolate(torch.tensor(gt).float().unsqueeze(0).unsqueeze(0), (512, 512))[0][0].int().numpy()\n",
    "            \n",
    "            frame_torch = image_to_torch(slce, device=device)\n",
    "            frame_torch = F.interpolate(frame_torch.unsqueeze(0), (480, 480))[0]\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                prediction = processor.step(frame_torch)\n",
    "\n",
    "            prediction = torch_prob_to_numpy_mask(prediction)\n",
    "\n",
    "            prediction = torch.tensor(prediction).float().unsqueeze(0).unsqueeze(0)\n",
    "            prediction = F.interpolate(prediction, (gt.shape[0], gt.shape[1]))[0]\n",
    "            if prediction.sum() == 0: break\n",
    "\n",
    "            box = ops.masks_to_boxes(prediction)[0]\n",
    "            \n",
    "            h, w = box[2] - box[0], box[3] - box[1]\n",
    "            y, x = (box[2] + box[0])/2, (box[3] + box[1])/2\n",
    "\n",
    "            pad = 1.1\n",
    "            box = np.array([y-h/2*pad, x-w/2*pad, y+h/2*pad, x+w/2*pad])\n",
    "\n",
    "            # raise Exception\n",
    "\n",
    "            predictor2.set_image(slce)\n",
    "            masks, scores, logits = predictor2.predict(\n",
    "                box = box,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            # visualize(slce, masks[0], gt, box, f\"{os.path.basename(di)}_{z}.jpg\")\n",
    "\n",
    "            mask_torch = torch.zeros(4, masks.shape[1], masks.shape[2]).cuda()\n",
    "            mask_torch[0] = torch.tensor(masks[0]).cuda()\n",
    "            mask_torch = F.interpolate(mask_torch.unsqueeze(0), (480, 480))[0]\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                # processor.clear_memory()\n",
    "                prediction = processor.step(frame_torch, mask_torch, idx_mask=False)\n",
    "\n",
    "            dices_tp += (gt*masks[0]).sum()\n",
    "            dices_pred += masks[0].sum()\n",
    "            dice_ = 2 * (gt*masks[0]).sum()/(gt.sum() + masks[0].sum())\n",
    "\n",
    "            if gt.sum() > 0:\n",
    "                dices.append(dice_)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            z += 1\n",
    "            \n",
    "        with torch.inference_mode():\n",
    "            processor.clear_memory()\n",
    "            prediction = processor.step(init_frame_torch, init_mask_torch, idx_mask=False)\n",
    "            \n",
    "        z = init_center[0] - 1\n",
    "        while z >= 0:\n",
    "            slce, gt = get_slice(img, mask, z)\n",
    "            \n",
    "            gt[gt!=(i+1)] = 0\n",
    "            gt[gt==(i+1)] = 1\n",
    "            # gt = F.interpolate(torch.tensor(gt).float().unsqueeze(0).unsqueeze(0), (512, 512))[0][0].int().numpy()\n",
    "            \n",
    "            frame_torch = image_to_torch(slce, device=device)\n",
    "            frame_torch = F.interpolate(frame_torch.unsqueeze(0), (480, 480))[0]\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                prediction = processor.step(frame_torch)\n",
    "\n",
    "            prediction = torch_prob_to_numpy_mask(prediction)\n",
    "\n",
    "            prediction = torch.tensor(prediction).float().unsqueeze(0).unsqueeze(0)\n",
    "            prediction = F.interpolate(prediction, (gt.shape[0], gt.shape[1]))[0]\n",
    "            if prediction.sum() == 0: break\n",
    "\n",
    "            box = ops.masks_to_boxes(prediction)[0]\n",
    "            \n",
    "            h, w = box[2] - box[0], box[3] - box[1]\n",
    "            y, x = (box[2] + box[0])/2, (box[3] + box[1])/2\n",
    "            \n",
    "            pad = 1.1\n",
    "            box = np.array([y-h/2*pad, x-w/2*pad, y+h/2*pad, x+w/2*pad])\n",
    "\n",
    "            predictor2.set_image(slce)\n",
    "            masks, scores, logits = predictor2.predict(\n",
    "                box = box,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            # visualize(slce, masks[0], gt, box, f\"{os.path.basename(di)}_{z}.jpg\")\n",
    "\n",
    "            mask_torch = torch.zeros(4, masks.shape[1], masks.shape[2]).cuda()\n",
    "            mask_torch[0] = torch.tensor(masks[0]).cuda()\n",
    "            mask_torch = F.interpolate(mask_torch.unsqueeze(0), (480, 480))[0]\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                # processor.clear_memory()\n",
    "                prediction = processor.step(frame_torch, mask_torch, idx_mask=False)\n",
    "\n",
    "            dices_tp += (gt*masks[0]).sum()\n",
    "            dices_pred += masks[0].sum()\n",
    "            dice_ = 2 * (gt*masks[0]).sum()/(gt.sum() + masks[0].sum())\n",
    "\n",
    "            if gt.sum() > 0:\n",
    "                dices.append(dice_)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            z -= 1\n",
    "    \n",
    "        dicesb_ = 2*dices_tp/(dices_pred+prop.area * (512/mask.shape[-1])**3)\n",
    "        \n",
    "        dicesb.append(dicesb_)\n",
    "\n",
    "        # raise Exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a835021",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices = np.array(dices)\n",
    "dicesb = np.array(dicesb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d109de-f4ef-4cfa-9620-4e797e5515db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8096056580110245\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.mean(dices))\n",
    "plt.hist(dices,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c5dd8-15b5-4bce-850d-630b15758682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
